\documentclass[a4paper,12pt]{article}
\usepackage[pdftex,bookmarks]{hyperref}
\usepackage{longtable}

\setlength\LTleft{0pt}
\setlength\LTright{0pt}

\begin{document}
\title{CM4all Beng Load Balancer}
\author{Max Kellermann}

\maketitle

\begin{abstract}
\emph{beng-lb} is a light-weight HTTP load balancer.
\end{abstract}

\setcounter{tocdepth}{2}
\tableofcontents
\newpage

\section{Features}

\emph{beng-lb} listens on a number of socket, and forwards the
requests to another server.  Behind one listener socket, more than one
cluster node may be configured.

The algorithm for selecting a cluster node is configurable: it may be
pure load balancing, pure failover, stickiness with the
\emph{beng-proxy} session cookie, or stickiness with a \emph{beng-lb}
cookie.


\section{Installation}

\emph{beng-lb} requires a current Debian operating system.

Install the package \texttt{cm4all-beng-lb}.  Edit the file
\texttt{/etc/cm4all/beng/lb.conf}.  Finally, restart
\emph{beng-lb}:

\begin{verbatim*}
/etc/init.d/cm4all-beng-lb restart
\end{verbatim*}

\section{Configuration}

Configuration is read from the file
\texttt{/etc/cm4all/beng/lb.conf}.  The following object types can be
configured here:

\begin{itemize}
\item nodes
\item pools
\item listeners
\end{itemize}

\subsection{Nodes}

A ``node'' is one server in a cluster.  It handles requests.  The node
has an IP address, but no port/service.  A node can have multiple
services, it can be member in multiple pools.  Technically, a physical
server can have multiple nodes by having more than one IP address, but
that is only a side effect and is not useful in practice.

Example:

\begin{verbatim*}
node localhost {
  address "127.0.0.1"
}
\end{verbatim*}

\subsection{Pools}

A pool is a cluster of nodes for a specific service.  When a request
needs to be handled, the pool chooses a member and sends the request
to this node.  Upon failure, it may repeat the request with a
different node.

Example:

\begin{verbatim*}
pool demo {
  protocol "http"
  member "foo:http"
  member "bar:http"
}
\end{verbatim*}

Instead of referring to a previously defined node name, you can
configure an IP address instead, and \texttt{beng-lb} creates a new
node implicitly.

The \verb|sticky| setting specifies how a node is chosen for a
request, see \ref{sticky} for details.

When all pool members fail, an error message is generated.  You can
override that behaviour by configuring a ``fallback'':

\begin{verbatim*}
pool demo {
  fallback "http://the.fallback.server/error.html"
  # ...
}
\end{verbatim*}

This would generate a ``320 Found'' redirect to the specified URL.
Another type of fallback is a custom response, you can specify a HTTP
status code and a brief message (plain text):

\begin{verbatim*}
pool demo {
  fallback "500" "Currently not available."
  # ...
}
\end{verbatim*}

The option \texttt{mangle\_via yes} enables request header mangling:
the headers \texttt{Via} and \texttt{X-Forwarded-For} are updated.

\subsubsection{Protocols}

The protocol \texttt{tcp} forwards raw a raw bidirectional TCP stream.
It is the fastest mode, and should be used when no special protocol
parsing is needed.

The protocol \texttt{http} means that \texttt{beng-lb} parses the
HTTP/1.1 request/response, and forwards them to the peer.  This HTTP
parser is needed for some of the advanced features, such as cookies.

\subsection{Listener}

A listener is a socket address (IP and port) which accepts incoming
TCP connections.

\begin{verbatim*}
listener port80 {
  bind "*:80"
  pool "demo"
}
\end{verbatim*}

A listener has a name, an socket address to bind to (including the
port).  To handle requests, it is associated with exactly one pool.

\subsection{Sticky}
\label{sticky}

A pool's \verb|sticky| setting specifies how a node is chosen for a
request.  Example:

\begin{verbatim*}
pool demo {
  protocol "http"
  member "foo:http"
  member "bar:http"
  sticky "failover"
}
\end{verbatim*}

Requests to this pool are always sent to the node named ``foo''.  The
second node ``bar'' is only used when ``foo'' fails.

Other sticky modes:

\begin{itemize}
\item \texttt{none}: simple round-robin (the default mode)
\item \texttt{failover}: the first non-failing node is used
\item \texttt{session\_modulo}: the modulo of the \texttt{beng-proxy}
  session is used to calculate the node
\item \texttt{cookie}: a random cookie is generated, and the node is
  chosen from the cookie that is received from the client
\item \verb|jvm_route|: Tomcat's JSESSIONID is parsed, and its suffix
  is compared against the \verb|jvm_route| of all member nodes
\end{itemize}

\subsubsection{Tomcat}

For the \verb|jvm_route| mode, both \emph{beng-lb} and Tomcat must be
configured properly.  Example \texttt{lb.conf}:

\begin{verbatim*}
node first {
   address 192.168.1.101
   jvm_route jvm1
}

node second {
   address 192.168.1.102
   jvm_route jvm2
}

pool demo {
  protocol "http"
  member "second:http"
  member "secondd:http"
  sticky "jvm_route"
}
\end{verbatim*}

Example \texttt{server.xml} on the ``first'' Tomcat:

\begin{verbatim*}
<Engine name="Catalina" defaultHost="localhost" jvmRoute="jvm1">
\end{verbatim*}

The \verb|jvmRoute| settings must match in \emph{beng-lb} and Tomcat.
It is allowed to set \verb|jvm_route| in a node that is used in pools
without the according \verb|sticky| setting.

\subsection{SSL/TLS}

To enable SSL/TLS on a listener, configure:

\begin{verbatim*}
listener ssl {
  bind "*:443"
  pool "demo"
  ssl "yes"
  ssl_cert "/etc/cm4all/beng/lb/cert.pem"
  ssl_key "/etc/cm4all/beng/lb/key.pem"
}
\end{verbatim*}

One pool can be shared by listeners with and without SSL.

The option \verb|ssl_verify| enables client certificates.  The subject
of the client certificate is copied to the web server in the
\verb|X-CM4all-BENG-Peer-Subject| request header, and its issuer uses
the \verb|X-CM4all-BENG-Peer-Issuer-Subject| request header.

\subsection{Monitors}

A ``monitor'' describes how the availability of nodes in a pool is
checked periodically.  By default, there is no monitor, just a list of
``known-bad'' nodes, filled with failures.

The option ``interval'' configures how often the monitor is executed
(in seconds).

\subsubsection{Ping}

The ``ping'' monitor periodically sends echo-request ICMP packets to
the node, and excepts echo-reply ICMP packets.

\begin{verbatim*}
monitor "my_monitor" {
  type "ping"
  interval "2"
}

pool "demo" {
  member "...
  monitor "my_monitor"
}
\end{verbatim*}

This requires Linux kernel 3.0 or newer.  \emph{beng-lb} must be
allowed to use the ICMP socket, which can be configured in the virtual
file:

\texttt{/proc/sys/net/ipv4/ping\_group\_range}

\subsubsection{Connect}

The ``connect'' monitor attempts to establish a TCP connection, and
closes it immediately.

\subsubsection{TCP Expect}

The ``tcp\_expect'' monitor opens a TCP connection, optionally sends
some data, and expects a certain string in the response.

\begin{verbatim*}
monitor "expect_monitor" {
  type "tcp_expect"
  send "GET / HTTP/1.1\r\nHost: localhost\r\n\r\n"
  expect "HTTP/1.1 200 OK"
}
\end{verbatim*}

The \texttt{send} setting is optional.

The \verb|expect_graceful| setting can be used for graceful shutdown:

\begin{verbatim*}
monitor "expect_monitor2" {
  type "tcp_expect"
  send "GET / HTTP/1.1\r\nHost: localhost\r\n\r\n"
  expect "HTTP/1.1 200 OK"
  expect_graceful "HTTP/1.1 503 Service Unavailable"
}
\end{verbatim*}

If the configured string is received, the node will only receive
``sticky'' requests, but no new sessions.  The \verb|expect| setting
is ignored if \verb|expect_graceful| was found, and \verb|expect| may
be omitted.

\end{document}
